{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Davis_simple_contrastive_loss_with_train_and_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkux9/X6NQ8yM3QF2EeBbA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLenthusiastic/context_contrastive_loss/blob/master/Davis_simple_contrastive_loss_with_train_and_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JanwPej3tX_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb4t0fyLta1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "logs_base_dir = \"runs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibSmT6B3tcg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 5777"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE5oZNWCte0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElwoNkUMtgfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('runs', 'zip', 'runs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8f7c96c5-e8be-41b8-93a4-ddd8995865d4",
        "id": "z5JzWgly_TUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns;sns.set()\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image, display\n",
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from argparse import ArgumentParser\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "\n",
        "parser = ArgumentParser(description='Simple Contrastive Loss')\n",
        "parser.add_argument('--batch_size', type=int, default=16)\n",
        "parser.add_argument('--constractive_loss_margin', type=float, default=0.8)\n",
        "parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "parser.add_argument('--num_epochs', type=int, default=100)\n",
        "parser.add_argument('--weight_decay', type=float, default=1e-5)\n",
        "parser.add_argument('--mode', type=str, default='train')\n",
        "parser.add_argument('--device', type=str, default='cuda')\n",
        "parser.add_argument('--validation_split', type=float, default=0.2)\n",
        "parser.add_argument('--img_size', type=int, default=200)\n",
        "parser.add_argument('--no_of_samples_per_class', type=int, default=100)\n",
        "parser.add_argument('--no_of_classes', type=int, default=50)\n",
        "parser.add_argument('--projector_img_size', type=int, default=32)\n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "DEVICE = args.device\n",
        "if not torch.cuda.is_available():\n",
        "    DEVICE = 'cpu'\n",
        "\n",
        "class SimaseDavis(Dataset):\n",
        "\n",
        "  def __init__(self, json_data, memmap):\n",
        "      self.memmap = memmap\n",
        "      self.json_data = json_data\n",
        "      self.shape = self.json_data[\"shape\"]\n",
        "      self.objects = self.json_data[\"objects\"]\n",
        "      self.classes = self.json_data[\"classes\"]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_object_1 = self.objects[str(index)]\n",
        "    img_object_1_width = img_object_1['width']\n",
        "    img_object_1_height = img_object_1['height']\n",
        "    image_1 =  self.memmap[index, :, :img_object_1_width, :img_object_1_height].astype(np.float32)\n",
        "    class_1 = img_object_1['class']\n",
        "\n",
        "\n",
        "    target = np.random.randint(0, 2)\n",
        "\n",
        "    if target == 0:  #similar classes\n",
        "        random_class_2 = np.random.choice(self.classes[class_1])\n",
        "        random_class_2_idx = random_class_2[\"object_idx\"]\n",
        "        if index == random_class_2_idx: #given index and random selected indexes are same\n",
        "           if len(self.classes[class_1]) != 1: #having only one object\n",
        "            temp_classes = copy.deepcopy(self.classes[class_1])\n",
        "            temp_classes.remove(random_class_2)\n",
        "            random_class_2_rand = np.random.choice(temp_classes)\n",
        "            random_class_2_idx = random_class_2_rand[\"object_idx\"]\n",
        "           else:\n",
        "            random_class_2_idx = random_class_2[\"object_idx\"]\n",
        "        class_2 = class_1\n",
        "        img_object_2 = self.objects[str(random_class_2_idx)]\n",
        "        img_object_2_width = img_object_2['width']\n",
        "        img_object_2_height = img_object_2['height']\n",
        "        image_2 =  self.memmap[random_class_2_idx, :, :img_object_2_width, :img_object_2_height].astype(np.float32)\n",
        "\n",
        "    else:\n",
        "      all_class_labels = copy.deepcopy(list(self.classes.keys()))\n",
        "      all_class_labels.remove(class_1)\n",
        "      class_2 = np.random.choice(all_class_labels)\n",
        "      img_object_2 = np.random.choice(self.classes[class_2])\n",
        "      img_object_2_idx = img_object_2[\"object_idx\"]\n",
        "      img_object_2_width = img_object_2[\"width\"]\n",
        "      img_object_2_height = img_object_2[\"height\"]\n",
        "      image_2 =  self.memmap[img_object_2_idx, :, :img_object_2_width, :img_object_2_height].astype(np.float32)\n",
        "    \n",
        "    image_1_tensor_ext = torch.from_numpy(image_1).unsqueeze(dim=0)\n",
        "    image_2_tensor_ext = torch.from_numpy(image_2).unsqueeze(dim=0)\n",
        "\n",
        "    image_1_tensor = F.interpolate(image_1_tensor_ext, size=(args.img_size,args.img_size))\n",
        "    image_2_tensor = F.interpolate(image_2_tensor_ext, size=(args.img_size,args.img_size))\n",
        "\n",
        "    return image_1_tensor, image_2_tensor, target, class_1, class_2\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.shape[0]\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        #conv and fc works as encoder\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                                  )\n",
        "        \n",
        "        # output 128, 21, 21 \n",
        "        self.fc = nn.Sequential(nn.Linear(128 * 21 * 21, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 128)\n",
        "                                )\n",
        "        \n",
        "    def forward(self, in1, in2):\n",
        "        x = torch.cat((in1, in2), dim=0)\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.fc(x)\n",
        "        z_out1, Z_out2 = torch.split(x, x.size(0) // 2, dim=0)\n",
        "        return z_out1, Z_out2\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        eq_distance = F.pairwise_distance(output[0], output[1])\n",
        "        loss = 0.5 * (1 - target) * torch.pow(eq_distance, 2) + \\\n",
        "               0.5 * target * torch.pow(torch.clamp(self.margin - eq_distance, min=0.00), 2)\n",
        "        return loss.mean()\n",
        "        \n",
        "folder_path = './drive/My Drive/'\n",
        "with open(folder_path+'train_davis.json') as json_file:\n",
        "    train_davis_json = json.load(json_file)\n",
        "with open(folder_path+'test_davis.json') as json_file:\n",
        "    test_davis_json = json.load(json_file)\n",
        "\n",
        "train_shape = train_davis_json[\"shape\"]\n",
        "train_memmap_path = folder_path+'train_davis.mmap'\n",
        "train_davis_memmap = np.memmap(train_memmap_path, dtype='uint8', mode='r', shape=tuple(train_shape))\n",
        "\n",
        "test_shape = test_davis_json[\"shape\"]\n",
        "test_memmap_path = folder_path+'test_davis.mmap'\n",
        "test_davis_memmap = np.memmap(test_memmap_path, dtype='uint8', mode='r', shape=tuple(test_shape))\n",
        "\n",
        "train_full_dataset = SimaseDavis(train_davis_json, train_davis_memmap)\n",
        "\n",
        "split = int(np.floor(args.validation_split * len(train_full_dataset)))\n",
        "indices = list(range(len(train_full_dataset)))\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "davis_train_loader = torch.utils.data.DataLoader(train_full_dataset, batch_size=args.batch_size,\n",
        "                                                   sampler=train_sampler,drop_last=True)\n",
        "davis_val_loader = torch.utils.data.DataLoader(train_full_dataset, batch_size=args.batch_size,\n",
        "                                                 sampler=valid_sampler, drop_last=True)\n",
        "\n",
        "#train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = args.batch_size, shuffle=True)\n",
        "\n",
        "tensorboard_writer = SummaryWriter()\n",
        "\n",
        "model_save_path = './drive/My Drive/encoder_3.pth'\n",
        "\n",
        "\n",
        "'''\n",
        "#showing pairs\n",
        "itf = next(iter(davis_dataloader))\n",
        "img1, img2, target, class_1, class_2 = itf\n",
        "img1 = img1.squeeze(dim=1)\n",
        "img2 = img2.squeeze(dim=1)\n",
        "\n",
        "for k in range(4):\n",
        "  i = img1[k].numpy().transpose(2,1,0)\n",
        "  cv2_imshow(i)\n",
        "  j = img2[k].numpy().transpose(2,1,0)\n",
        "  cv2_imshow(j)\n",
        "'''\n",
        "\n",
        "\n",
        "encoder = Encoder()\n",
        "encoder = encoder.to(DEVICE)\n",
        "\n",
        "criterion = ContrastiveLoss(margin=args.constractive_loss_margin)\n",
        "optimizer = torch.optim.Adam(params=encoder.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "\n",
        "print('training started')\n",
        "\n",
        "epoches = []\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "def transform_image_for_projector(img_tensor):\n",
        "\n",
        "  x_np = img_tensor.to('cpu').data.numpy()\n",
        "  x_np = x_np.swapaxes(0, 1)\n",
        "  x_np = x_np.swapaxes(1, 2)\n",
        "  # H, W, C\n",
        "  img = Image.fromarray(x_np.astype(np.uint8), mode='RGB')\n",
        "\n",
        "  img = img.resize((args.projector_img_size, args.projector_img_size), Image.ANTIALIAS)\n",
        "  img = np.array(img).astype(np.float)\n",
        "\n",
        "  img = img.swapaxes(2, 1)\n",
        "  img = img.swapaxes(1, 0)\n",
        "  img /= 255\n",
        "    \n",
        "  return img\n",
        "\n",
        "def draw_loss_plot(training_losses, validation_losses, epochs):\n",
        "    plt.plot(epochs, training_losses, label=\"Train\")\n",
        "    plt.plot(epochs, validation_losses, label=\"eval\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for epoch in range(1,args.num_epochs+1):\n",
        "\n",
        "  epoches.append(epoch)\n",
        "  train_batch_losses = []\n",
        "  eval_batch_losses = []\n",
        "  stage = ''\n",
        "\n",
        "  for dataloader in [davis_train_loader,davis_val_loader]:\n",
        "\n",
        "    if dataloader == davis_train_loader:\n",
        "      encoder.train()\n",
        "      torch.set_grad_enabled(True)\n",
        "      stage = 'train'\n",
        "    else:\n",
        "      encoder.eval()\n",
        "      torch.set_grad_enabled(False)\n",
        "      stage = 'eval'\n",
        " \n",
        "    for batch in dataloader:\n",
        "\n",
        "        img_objects_1, img_objects_2, target , class_1, class_2 = batch\n",
        "        \n",
        "        img_objects_1_t = img_objects_1.squeeze(dim=1)\n",
        "        img_objects_2_t = img_objects_2.squeeze(dim=1)\n",
        "        img_obj_1 = img_objects_1_t.to(DEVICE)\n",
        "        img_obj_2 = img_objects_2_t.to(DEVICE)\n",
        "    \n",
        "        z_out1, z_out2 = encoder(img_obj_1, img_obj_2)\n",
        "\n",
        "        z_out = [z_out1, z_out2]\n",
        "\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        loss = criterion(z_out, target)\n",
        "\n",
        "        if dataloader == davis_train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_batch_losses.append(loss.item())\n",
        "        else:\n",
        "          eval_batch_losses.append(loss.item())\n",
        "\n",
        "    if dataloader == davis_train_loader:    \n",
        "      print('Epoch : ', epoch,'Stage : ', stage, 'Loss : ', np.mean(train_batch_losses))\n",
        "      train_losses.append(np.mean(train_batch_losses))\n",
        "      tensorboard_writer.add_scalar(scalar_value=np.mean(train_batch_losses), global_step=epoch, tag=f'{stage}_loss')\n",
        "      torch.save(encoder.state_dict(), model_save_path)\n",
        "    else:\n",
        "      print('Epoch : ', epoch,'Stage : ', stage, 'Loss : ', np.mean(eval_batch_losses))\n",
        "      eval_losses.append(np.mean(eval_batch_losses))\n",
        "      tensorboard_writer.add_scalar(scalar_value=np.mean(train_batch_losses), global_step=epoch, tag=f'{stage}_loss')\n",
        "\n",
        "  #adding to tensorboard projector\n",
        "\n",
        "  encoder.eval()\n",
        "  torch.set_grad_enabled(False)\n",
        "\n",
        "  classes_dict = {}\n",
        "  projector_labels = []\n",
        "  projector_imgs = []\n",
        "  projector_embeddings = []\n",
        "\n",
        "  for dataloader in [davis_train_loader,davis_val_loader]:\n",
        "    \n",
        "    if dataloader == davis_train_loader:\n",
        "       stage = 'train'\n",
        "    else:\n",
        "      stage = 'eval'\n",
        "  \n",
        "    for batch in dataloader:\n",
        "\n",
        "      img_objects_1, img_objects_2, target , class_1, class_2 = batch\n",
        "\n",
        "      img_objects_1_t = img_objects_1.squeeze(dim=1)\n",
        "      img_objects_2_t = img_objects_2.squeeze(dim=1)\n",
        "      img_obj_1 = img_objects_1_t.to(DEVICE)\n",
        "      img_obj_2 = img_objects_2_t.to(DEVICE)\n",
        "\n",
        "      z_out1, z_out2 = encoder(img_obj_1, img_obj_2)\n",
        "\n",
        "      counter = 0\n",
        "      for classes in [class_1, class_2]:\n",
        "        if len(classes_dict.keys()) <= args.no_of_classes:\n",
        "          for idx, single_class in enumerate(classes):\n",
        "            counter += 1 \n",
        "            if single_class not in classes_dict.keys():\n",
        "              classes_dict[single_class] = 1\n",
        "              projector_labels.append(single_class)\n",
        "              if counter/args.batch_size == 0 :\n",
        "                projector_imgs.append(transform_image_for_projector(img_objects_1_t[idx]))\n",
        "                projector_embeddings.append(z_out1.cpu()[idx])\n",
        "              else:\n",
        "                projector_imgs.append(transform_image_for_projector(img_objects_2_t[idx]))\n",
        "                projector_embeddings.append(z_out2.cpu()[idx])\n",
        "            else:\n",
        "              current_count = classes_dict.get(single_class)\n",
        "              if current_count <= args.no_of_samples_per_class:\n",
        "                  classes_dict[single_class] = current_count+1\n",
        "                  projector_labels.append(single_class)\n",
        "                  if counter/args.batch_size == 0 :\n",
        "                    projector_imgs.append(transform_image_for_projector(img_objects_1_t[idx]))\n",
        "                    projector_embeddings.append(z_out1.cpu()[idx])\n",
        "                  else:\n",
        "                    projector_imgs.append(transform_image_for_projector(img_objects_2_t[idx]))\n",
        "                    projector_embeddings.append(z_out2.cpu()[idx])\n",
        "  \n",
        "    \n",
        "    tensorboard_writer.add_embedding(\n",
        "                    mat=torch.FloatTensor(np.stack(projector_embeddings)),\n",
        "                    label_img=torch.FloatTensor(np.stack(projector_imgs)),\n",
        "                    metadata=projector_labels,\n",
        "                    global_step=epoch, tag=f'{stage}_emb_{epoch}')\n",
        "  \n",
        "  #tensorboard_writer.add_scalar(scalar_value=np.mean(batch_losses), global_step=epoch, tag=f'{stage}_loss' )\n",
        "    \n",
        "draw_loss_plot(train_losses, eval_losses, epoches)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training started\n",
            "Epoch :  1 Stage :  train Loss :  0.2607183829225876\n",
            "Epoch :  1 Stage :  eval Loss :  0.07332958027043125\n",
            "Epoch :  2 Stage :  train Loss :  0.059276997601544416\n",
            "Epoch :  2 Stage :  eval Loss :  0.057454876601696014\n",
            "Epoch :  3 Stage :  train Loss :  0.051773367863562375\n",
            "Epoch :  3 Stage :  eval Loss :  0.057102604007179085\n",
            "Epoch :  4 Stage :  train Loss :  0.04239523121052318\n",
            "Epoch :  4 Stage :  eval Loss :  0.05426837261201757\n",
            "Epoch :  5 Stage :  train Loss :  0.03386269969934667\n",
            "Epoch :  5 Stage :  eval Loss :  0.04367753186009147\n",
            "Epoch :  6 Stage :  train Loss :  0.027918340831443116\n",
            "Epoch :  6 Stage :  eval Loss :  0.03958353363541943\n",
            "Epoch :  7 Stage :  train Loss :  0.023830395247097368\n",
            "Epoch :  7 Stage :  eval Loss :  0.029674876961066868\n",
            "Epoch :  8 Stage :  train Loss :  0.0187325999589154\n",
            "Epoch :  8 Stage :  eval Loss :  0.04791011057342544\n",
            "Epoch :  9 Stage :  train Loss :  0.0169270273608466\n",
            "Epoch :  9 Stage :  eval Loss :  0.02630318955264308\n",
            "Epoch :  10 Stage :  train Loss :  0.01137818439208247\n",
            "Epoch :  10 Stage :  eval Loss :  0.02394208321206723\n",
            "Epoch :  11 Stage :  train Loss :  0.00923091479646111\n",
            "Epoch :  11 Stage :  eval Loss :  0.017901081464846025\n",
            "Epoch :  12 Stage :  train Loss :  0.007715852362862615\n",
            "Epoch :  12 Stage :  eval Loss :  0.019213942211177764\n",
            "Epoch :  13 Stage :  train Loss :  0.006544038624054304\n",
            "Epoch :  13 Stage :  eval Loss :  0.020482952170299763\n",
            "Epoch :  14 Stage :  train Loss :  0.005505591542977426\n",
            "Epoch :  14 Stage :  eval Loss :  0.017690825974568725\n",
            "Epoch :  15 Stage :  train Loss :  0.005758870111916352\n",
            "Epoch :  15 Stage :  eval Loss :  0.018377850304893924\n",
            "Epoch :  16 Stage :  train Loss :  0.004185167120131492\n",
            "Epoch :  16 Stage :  eval Loss :  0.013314810179343278\n",
            "Epoch :  17 Stage :  train Loss :  0.0037194611198544778\n",
            "Epoch :  17 Stage :  eval Loss :  0.014396846132127173\n",
            "Epoch :  18 Stage :  train Loss :  0.0035729958699515984\n",
            "Epoch :  18 Stage :  eval Loss :  0.012825632601919951\n",
            "Epoch :  19 Stage :  train Loss :  0.0024536597561867286\n",
            "Epoch :  19 Stage :  eval Loss :  0.012362797317715982\n",
            "Epoch :  20 Stage :  train Loss :  0.001989546685406283\n",
            "Epoch :  20 Stage :  eval Loss :  0.010833772065146177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e8d4c54d5312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;31m#tensorboard_writer.add_scalar(scalar_value=np.mean(batch_losses), global_step=epoch, tag=f'{stage}_loss' )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m \u001b[0mdraw_loss_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_batch_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e8d4c54d5312>\u001b[0m in \u001b[0;36mdraw_loss_plot\u001b[0;34m(training_losses, validation_losses, epochs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_loss_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \"\"\"\n\u001b[1;32m   1645\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    343\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (33,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}