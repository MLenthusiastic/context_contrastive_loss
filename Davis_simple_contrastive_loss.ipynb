{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Davis_simple_contrastive_loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNctTvH/nbe7oWVZE93n3tK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLenthusiastic/context_contrastive_loss/blob/master/Davis_simple_contrastive_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWYwD6HLsaUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6MhQ7V5sbZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "logs_base_dir = \"runs\"\n",
        "os.makedirs(logs_base_dir, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcUH_jzqsd59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 5777"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_rDIegsshGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvbH0xx8si8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('runs', 'zip', 'runs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65nIW7j2kISQ",
        "colab_type": "code",
        "outputId": "4c556367-2dd9-42d5-aa29-18ed8dae5770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns;sns.set()\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import Image, display\n",
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from argparse import ArgumentParser\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "\n",
        "parser = ArgumentParser(description='Simple Contrastive Loss')\n",
        "parser.add_argument('--batch_size', type=int, default=16)\n",
        "parser.add_argument('--constractive_loss_margin', type=float, default=0.8)\n",
        "parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "parser.add_argument('--num_epochs', type=int, default=100)\n",
        "parser.add_argument('--weight_decay', type=float, default=1e-5)\n",
        "parser.add_argument('--mode', type=str, default='test')\n",
        "parser.add_argument('--device', type=str, default='cuda')\n",
        "parser.add_argument('--img_size', type=int, default=200)\n",
        "parser.add_argument('--no_of_samples_per_class', type=int, default=100)\n",
        "parser.add_argument('--no_of_classes', type=int, default=50)\n",
        "parser.add_argument('--projector_img_size', type=int, default=32)\n",
        "\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "DEVICE = args.device\n",
        "if not torch.cuda.is_available():\n",
        "    DEVICE = 'cpu'\n",
        "\n",
        "class SimaseDavis(Dataset):\n",
        "\n",
        "  def __init__(self, json_data, memmap):\n",
        "      self.memmap = memmap\n",
        "      self.json_data = json_data\n",
        "      self.shape = self.json_data[\"shape\"]\n",
        "      self.objects = self.json_data[\"objects\"]\n",
        "      self.classes = self.json_data[\"classes\"]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img_object_1 = self.objects[str(index)]\n",
        "    img_object_1_width = img_object_1['width']\n",
        "    img_object_1_height = img_object_1['height']\n",
        "    image_1 =  self.memmap[index, :, :img_object_1_width, :img_object_1_height].astype(np.float32)\n",
        "    class_1 = img_object_1['class']\n",
        "\n",
        "\n",
        "    target = np.random.randint(0, 2)\n",
        "\n",
        "    if target == 0:  #similar classes\n",
        "        random_class_2 = np.random.choice(self.classes[class_1])\n",
        "        random_class_2_idx = random_class_2[\"object_idx\"]\n",
        "        if index == random_class_2_idx: #given index and random selected indexes are same\n",
        "           if len(self.classes[class_1]) != 1: #having only one object\n",
        "            temp_classes = copy.deepcopy(self.classes[class_1])\n",
        "            temp_classes.remove(random_class_2)\n",
        "            random_class_2_rand = np.random.choice(temp_classes)\n",
        "            random_class_2_idx = random_class_2_rand[\"object_idx\"]\n",
        "           else:\n",
        "            random_class_2_idx = random_class_2[\"object_idx\"]\n",
        "        class_2 = class_1\n",
        "        img_object_2 = self.objects[str(random_class_2_idx)]\n",
        "        img_object_2_width = img_object_2['width']\n",
        "        img_object_2_height = img_object_2['height']\n",
        "        image_2 =  self.memmap[random_class_2_idx, :, :img_object_2_width, :img_object_2_height].astype(np.float32)\n",
        "\n",
        "    else:\n",
        "      all_class_labels = copy.deepcopy(list(self.classes.keys()))\n",
        "      all_class_labels.remove(class_1)\n",
        "      class_2 = np.random.choice(all_class_labels)\n",
        "      img_object_2 = np.random.choice(self.classes[class_2])\n",
        "      img_object_2_idx = img_object_2[\"object_idx\"]\n",
        "      img_object_2_width = img_object_2[\"width\"]\n",
        "      img_object_2_height = img_object_2[\"height\"]\n",
        "      image_2 =  self.memmap[img_object_2_idx, :, :img_object_2_width, :img_object_2_height].astype(np.float32)\n",
        "    \n",
        "    image_1_tensor_ext = torch.from_numpy(image_1).unsqueeze(dim=0)\n",
        "    image_2_tensor_ext = torch.from_numpy(image_2).unsqueeze(dim=0)\n",
        "\n",
        "    image_1_tensor = F.interpolate(image_1_tensor_ext, size=(args.img_size,args.img_size))\n",
        "    image_2_tensor = F.interpolate(image_2_tensor_ext, size=(args.img_size,args.img_size))\n",
        "\n",
        "    return image_1_tensor, image_2_tensor, target, class_1, class_2\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.shape[0]\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        #conv and fc works as encoder\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                                  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "                                  )\n",
        "        \n",
        "        # output 128, 21, 21 \n",
        "        self.fc = nn.Sequential(nn.Linear(128 * 21 * 21, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 1024),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(1024, 128)\n",
        "                                )\n",
        "        \n",
        "    def forward(self, in1, in2):\n",
        "        x = torch.cat((in1, in2), dim=0)\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.fc(x)\n",
        "        z_out1, Z_out2 = torch.split(x, x.size(0) // 2, dim=0)\n",
        "        return z_out1, Z_out2\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, margin):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        eq_distance = F.pairwise_distance(output[0], output[1])\n",
        "        loss = 0.5 * (1 - target) * torch.pow(eq_distance, 2) + \\\n",
        "               0.5 * target * torch.pow(torch.clamp(self.margin - eq_distance, min=0.00), 2)\n",
        "        return loss.mean()\n",
        "        \n",
        "folder_path = './drive/My Drive/'\n",
        "with open(folder_path+'train_davis.json') as json_file:\n",
        "     train_davis_json = json.load(json_file)\n",
        "with open(folder_path+'test_davis.json') as json_file:\n",
        "     test_davis_json = json.load(json_file)\n",
        "\n",
        "train_shape = train_davis_json[\"shape\"]\n",
        "train_memmap_path = folder_path+'train_davis.mmap'\n",
        "train_davis_memmap = np.memmap(train_memmap_path, dtype='uint8', mode='r', shape=tuple(train_shape))\n",
        "\n",
        "test_shape = test_davis_json[\"shape\"]\n",
        "test_memmap_path = folder_path+'test_davis.mmap'\n",
        "test_davis_memmap = np.memmap(test_memmap_path, dtype='uint8', mode='r', shape=tuple(test_shape))\n",
        "\n",
        "train_davis_dataset = SimaseDavis(train_davis_json,train_davis_memmap)\n",
        "train_davis_dataloader = torch.utils.data.DataLoader(train_davis_dataset, batch_size = args.batch_size, shuffle=True)\n",
        "\n",
        "test_davis_dataset = SimaseDavis(test_davis_json,test_davis_memmap)\n",
        "test_davis_dataloader = torch.utils.data.DataLoader(test_davis_dataset, batch_size = args.batch_size, shuffle=True)\n",
        "\n",
        "tensorboard_writer = SummaryWriter()\n",
        "\n",
        "'''\n",
        "\n",
        "#showing pairs\n",
        "itf = next(iter(davis_dataloader))\n",
        "img1, img2, target, class_1, class_2 = itf\n",
        "img1 = img1.squeeze(dim=1)\n",
        "img2 = img2.squeeze(dim=1)\n",
        "\n",
        "for k in range(4):\n",
        "  i = img1[k].numpy().transpose(2,1,0)\n",
        "  cv2_imshow(i)\n",
        "  j = img2[k].numpy().transpose(2,1,0)\n",
        "  cv2_imshow(j)\n",
        "\n",
        "'''\n",
        "encoder = Encoder()\n",
        "encoder = encoder.to(DEVICE)\n",
        "\n",
        "criterion = ContrastiveLoss(margin=args.constractive_loss_margin)\n",
        "optimizer = torch.optim.Adam(params=encoder.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "\n",
        "print('training started')\n",
        "model_save_path = './drive/My Drive/encoder_3.pth'\n",
        "\n",
        "epoches = []\n",
        "train_losses = []\n",
        "\n",
        "def transform_image_for_projector(img_tensor):\n",
        "\n",
        "  x_np = img_tensor.to('cpu').data.numpy()\n",
        "  x_np = x_np.swapaxes(0, 1)\n",
        "  x_np = x_np.swapaxes(1, 2)\n",
        "  # H, W, C\n",
        "  img = Image.fromarray(x_np.astype(np.uint8), mode='RGB')\n",
        "\n",
        "  img = img.resize((args.projector_img_size, args.projector_img_size), Image.ANTIALIAS)\n",
        "  img = np.array(img).astype(np.float)\n",
        "\n",
        "  img = img.swapaxes(2, 1)\n",
        "  img = img.swapaxes(1, 0)\n",
        "  img /= 255\n",
        "    \n",
        "  return img\n",
        "\n",
        "def draw_loss_plot(training_losses, epochs):\n",
        "    plt.plot(epochs, training_losses, label=\"Train\")\n",
        "    #plt.plot(epochs, validation_losses, label=\"eval\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if args.mode == 'train':\n",
        "\n",
        "  for epoch in range(1,args.num_epochs+1):\n",
        "\n",
        "    epoches.append(epoch)\n",
        "    batch_losses = []\n",
        "\n",
        "    encoder.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "  \n",
        "    for batch in train_davis_dataloader:\n",
        "        img_objects_1, img_objects_2, target , class_1, class_2 = batch\n",
        "        \n",
        "        img_objects_1_t = img_objects_1.squeeze(dim=1)\n",
        "        img_objects_2_t = img_objects_2.squeeze(dim=1)\n",
        "        img_obj_1 = img_objects_1_t.to(DEVICE)\n",
        "        img_obj_2 = img_objects_2_t.to(DEVICE)\n",
        "    \n",
        "        z_out1, z_out2 = encoder(img_obj_1, img_obj_2)\n",
        "\n",
        "        z_out = [z_out1, z_out2]\n",
        "\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        loss = criterion(z_out, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss.item())\n",
        "    \n",
        "    print('Epoch : ', epoch, 'Loss : ', np.mean(batch_losses))\n",
        "    train_losses.append(np.mean(batch_losses))\n",
        "    torch.save(encoder.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "    #adding to tensorboard projector\n",
        "\n",
        "    encoder.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    classes_dict = {}\n",
        "    projector_labels = []\n",
        "    projector_imgs = []\n",
        "    projector_embeddings = []\n",
        "\n",
        "    for batch in train_davis_dataloader:\n",
        "\n",
        "      img_objects_1, img_objects_2, target , class_1, class_2 = batch\n",
        "\n",
        "      img_objects_1_t = img_objects_1.squeeze(dim=1)\n",
        "      img_objects_2_t = img_objects_2.squeeze(dim=1)\n",
        "      img_obj_1 = img_objects_1_t.to(DEVICE)\n",
        "      img_obj_2 = img_objects_2_t.to(DEVICE)\n",
        "\n",
        "      z_out1, z_out2 = encoder(img_obj_1, img_obj_2)\n",
        "\n",
        "      counter = 0\n",
        "      for classes in [class_1, class_2]:\n",
        "        if len(classes_dict.keys()) <= args.no_of_classes:\n",
        "          for idx, single_class in enumerate(classes):\n",
        "            counter += 1 \n",
        "            if single_class not in classes_dict.keys():\n",
        "              classes_dict[single_class] = 1\n",
        "              projector_labels.append(single_class)\n",
        "              if counter/args.batch_size == 0 :\n",
        "                projector_imgs.append(transform_image_for_projector(img_objects_1_t[idx]))\n",
        "                projector_embeddings.append(z_out1.cpu()[idx])\n",
        "              else:\n",
        "                projector_imgs.append(transform_image_for_projector(img_objects_2_t[idx]))\n",
        "                projector_embeddings.append(z_out2.cpu()[idx])\n",
        "            else:\n",
        "              current_count = classes_dict.get(single_class)\n",
        "              if current_count <= args.no_of_samples_per_class:\n",
        "                  classes_dict[single_class] = current_count+1\n",
        "                  projector_labels.append(single_class)\n",
        "                  if counter/args.batch_size == 0 :\n",
        "                    projector_imgs.append(transform_image_for_projector(img_objects_1_t[idx]))\n",
        "                    projector_embeddings.append(z_out1.cpu()[idx])\n",
        "                  else:\n",
        "                    projector_imgs.append(transform_image_for_projector(img_objects_2_t[idx]))\n",
        "                    projector_embeddings.append(z_out2.cpu()[idx])\n",
        "\n",
        "    tensorboard_writer.add_embedding(\n",
        "                      mat=torch.FloatTensor(np.stack(projector_embeddings)),\n",
        "                      label_img=torch.FloatTensor(np.stack(projector_imgs)),\n",
        "                      metadata=projector_labels,\n",
        "                      global_step=epoch, tag=f'train_emb_{epoch}')\n",
        "    tensorboard_writer.add_scalar(scalar_value=np.mean(batch_losses), global_step=epoch, tag=f'train_loss' )\n",
        "\n",
        "    \n",
        "        \n",
        "  draw_loss_plot(train_losses,epoches)\n",
        "\n",
        "elif args.mode == 'test':\n",
        "\n",
        "    #load previously trained encoder model\n",
        "    model_path = './drive/My Drive/encoder_3.pth'\n",
        "    encoder_model = Encoder()\n",
        "    encoder_model.load_state_dict(torch.load(model_path))\n",
        "    encoder_model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "\n",
        "    classes_dict = {}\n",
        "    projector_labels = []\n",
        "    projector_imgs = []\n",
        "    projector_embeddings = []\n",
        "\n",
        "    for batch in test_davis_dataloader:\n",
        "\n",
        "      img_objects_1, img_objects_2, target , class_1, class_2 = batch\n",
        "\n",
        "      img_objects_1_t = img_objects_1.squeeze(dim=1)\n",
        "      img_objects_2_t = img_objects_2.squeeze(dim=1)\n",
        "      img_obj_1 = img_objects_1_t.to(DEVICE)\n",
        "      img_obj_2 = img_objects_2_t.to(DEVICE)\n",
        "\n",
        "      z_out1, z_out2 = encoder(img_obj_1, img_obj_2)\n",
        "\n",
        "      counter = 0\n",
        "      for classes in [class_1, class_2]:\n",
        "        if len(classes_dict.keys()) <= args.no_of_classes:\n",
        "          for idx, single_class in enumerate(classes):\n",
        "            counter += 1 \n",
        "            if single_class not in classes_dict.keys():\n",
        "              classes_dict[single_class] = 1\n",
        "              projector_labels.append(single_class)\n",
        "              if counter/args.batch_size == 0 :\n",
        "                projector_imgs.append(transform_image_for_projector(img_objects_1_t[idx]))\n",
        "                projector_embeddings.append(z_out1.cpu()[idx])\n",
        "              else:\n",
        "                projector_imgs.append(transform_image_for_projector(img_objects_2_t[idx]))\n",
        "                projector_embeddings.append(z_out2.cpu()[idx])\n",
        "            else:\n",
        "              current_count = classes_dict.get(single_class)\n",
        "              if current_count <= args.no_of_samples_per_class:\n",
        "                  classes_dict[single_class] = current_count+1\n",
        "                  projector_labels.append(single_class)\n",
        "                  if counter/args.batch_size == 0 :\n",
        "                    projector_imgs.append(transform_image_for_projector(img_objects_1_t[idx]))\n",
        "                    projector_embeddings.append(z_out1.cpu()[idx])\n",
        "                  else:\n",
        "                    projector_imgs.append(transform_image_for_projector(img_objects_2_t[idx]))\n",
        "                    projector_embeddings.append(z_out2.cpu()[idx])\n",
        "\n",
        "    tensorboard_writer.add_embedding(\n",
        "                      mat=torch.FloatTensor(np.stack(projector_embeddings)),\n",
        "                      label_img=torch.FloatTensor(np.stack(projector_imgs)),\n",
        "                      metadata=projector_labels,\n",
        "                      global_step=1, tag=f'test_emb_1')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training started\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}